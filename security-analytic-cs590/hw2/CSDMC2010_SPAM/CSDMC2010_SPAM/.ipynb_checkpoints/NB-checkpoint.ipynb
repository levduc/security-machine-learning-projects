{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "import nltk\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "# from nltk.corpus import wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('snowball')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing: Label\n",
    "def preProcessingLabel(labelFile = \"SPAM.label\"):\n",
    "    labelDict = {}\n",
    "    hamCount = 0\n",
    "    spamCount = 0\n",
    "    with open(\"SPAM.label\") as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            labelDict[key] = int(val)\n",
    "            # only consider TRAINING label\n",
    "            if 'TRAIN_' in key:\n",
    "                if int(val) == 1: \n",
    "                    hamCount +=1\n",
    "                else:\n",
    "                    spamCount +=1\n",
    "    f.close()\n",
    "    return hamCount, spamCount, labelDict\n",
    "### Preprocessing: building vocabulary + frequency:\n",
    "def buildingVocabulary(trainingDir = 'processed_traning'):\n",
    "    # read spam label to identify which doc is spam\n",
    "    (hamCount, spamCount, labelDict) = preProcessingLabel()    \n",
    "    # for each file in the training directory\n",
    "    onlyfiles = [f for f in listdir(trainingDir) if isfile(join(trainingDir, f))]\n",
    "    wordList = set()     # vocabulary\n",
    "    spamWordDict = {}    # frequency in spam doc\n",
    "    hamWordDict = {}     # frequency in ham doc\n",
    "    stop_words = set(stopwords.words('english'))        #english stop words\n",
    "    for i in onlyfiles:\n",
    "        #  ignore random dir\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        inputFile = trainingDir+'/'+i\n",
    "        s = open(inputFile,encoding=\"latin-1\").read()\n",
    "        tokens = s.split()\n",
    "        for word in tokens:\n",
    "            ### Processing word here##################\n",
    "            # ignore stopwords\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if word in ':;.,/\\(\\)\\[\\]<>': # remove normal character\n",
    "                continue\n",
    "            # lemmatize word \n",
    "            lmWord = word\n",
    "            ##########################################\n",
    "            ### Update vocabulary#####################\n",
    "            if lmWord in wordList:\n",
    "                # if in ham, update ham\n",
    "                if labelDict[i] == 1: # ham\n",
    "                    if lmWord in hamWordDict:\n",
    "                        hamWordDict[lmWord] += 1\n",
    "                    else:\n",
    "                        hamWordDict[lmWord] = 1\n",
    "                # if in spam, update spam\n",
    "                if labelDict[i] == 0: # spam\n",
    "                    if lmWord in spamWordDict:\n",
    "                        spamWordDict[lmWord] += 1\n",
    "                    else:\n",
    "                        spamWordDict[lmWord] = 1\n",
    "            else:\n",
    "                # add new word to word list.\n",
    "                wordList.add(lmWord)\n",
    "                # if in ham, update ham\n",
    "                if labelDict[i] == 1: # ham\n",
    "                    if lmWord in hamWordDict:\n",
    "                        hamWordDict[lmWord] += 1\n",
    "                    else:\n",
    "                        hamWordDict[lmWord] = 1\n",
    "                # if in spam, update spam\n",
    "                if labelDict[i] == 0: # spam\n",
    "                    if lmWord in spamWordDict:\n",
    "                        spamWordDict[lmWord] += 1\n",
    "                    else:\n",
    "                        spamWordDict[lmWord] = 1\n",
    "            ##########################################\n",
    "    return wordList, spamWordDict, hamWordDict\n",
    "# Naive Bayes for each file \n",
    "def NBeachFile(fileName,hamCount,spamCount,vocabulary,spamWordDict,hamWordDict):\n",
    "    spamWordCount = sum(spamWordDict.values())   #count word freq in spam\n",
    "    hamWordCount = sum(hamWordDict.values())     #count word freq in ham\n",
    "    vocabSize = len(vocabulary)\n",
    "    stop_words = set(stopwords.words('english')) #english stop words\n",
    "    spamProb = math.log(spamCount/(hamCount+spamCount))\n",
    "    hamProb = math.log(hamCount/(hamCount+spamCount))\n",
    "    s = open(fileName,encoding=\"latin-1\").read()\n",
    "    tokens = s.split()\n",
    "    for word in tokens:\n",
    "            ### Processing word here##################\n",
    "            # ignore stopwords\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if word in ':;.,/\\(\\)\\[\\]<>': # some normal character\n",
    "                continue\n",
    "            lmWord = word # was thinking about lemmatize word\n",
    "            ##########################################\n",
    "            ### compute spamscore\n",
    "            freqInSpam = 0\n",
    "            freqInHam = 0\n",
    "            if lmWord in spamWordDict:\n",
    "                freqInSpam = spamWordDict[lmWord]\n",
    "            else:\n",
    "                freqInSpam = 0\n",
    "            if lmWord in hamWordDict:\n",
    "                freqInHam = hamWordDict[lmWord]\n",
    "            else:\n",
    "                freqInHam = 0\n",
    "            # log to avoid becoming 0\n",
    "            spamProb += math.log((freqInSpam+1)/(vocabSize+spamWordCount))\n",
    "            hamProb += math.log((freqInHam+1)/(vocabSize+hamWordCount))\n",
    "    if hamProb > spamProb:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# Naive bayes implementation\n",
    "def NB(testDir, vocabulary,spamWordDict,hamWordDict):\n",
    "    # read spam label to identify which doc is spam\n",
    "    (hamCount, spamCount, labelDict) = preProcessingLabel()\n",
    "    # for each file in the testing directory\n",
    "    onlyfiles = [f for f in listdir(testDir) if isfile(join(testDir, f))]\n",
    "    tp = 0 # true positive\n",
    "    fp = 0 # false postive\n",
    "    fn = 0 # false negative\n",
    "    tn = 0 # true negative\n",
    "    for i in onlyfiles:\n",
    "        if i == '.DS_Store':\n",
    "            continue\n",
    "        inputFile = testDir+'/'+i\n",
    "        ### use naive bayes to get prediction\n",
    "        score = NBeachFile(inputFile,hamCount,spamCount,vocabulary,spamWordDict,hamWordDict)\n",
    "        ### compute stat base on labelDict\n",
    "        if score == 1:\n",
    "            if labelDict[i] == 1:  # label 1 as positive\n",
    "                tp += 1\n",
    "            else: \n",
    "                fp += 1\n",
    "        else:\n",
    "            if labelDict[i] == 0:  # label 0 as negative\n",
    "                tn += 1\n",
    "            else:\n",
    "                fn += 1\n",
    "    return tp,fp,fn,tn\n",
    "### simple stat\n",
    "def computeStat(tp,fp,fn,tn):\n",
    "    print('False Positive Rate:\\t', fp)\n",
    "    print('False Negative Rate:\\t', fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    print('Recall:\\t\\t\\t', recall)    \n",
    "    print('Precision:\\t\\t',precision)\n",
    "    fscore = 2*precision*recall/(precision + recall)\n",
    "    print('F-score beta = 1: \\t', fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False Positive Rate:\t 28\n",
      "False Negative Rate:\t 8\n",
      "Recall:\t\t\t 0.9911699779249448\n",
      "Precision:\t\t 0.9697624190064795\n",
      "F-score beta = 1: \t 0.980349344978166\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "(vocabulary,spamWordDict,hamWordDict) = buildingVocabulary('processed_training')\n",
    "# perform NB on test data and compare with label\n",
    "(tp,fp,fn,tn)=NB('processed_testing',vocabulary,spamWordDict,hamWordDict)\n",
    "# output stat \n",
    "computeStat(tp,fp,fn,tn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hamCount' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-52f3f8e029da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mNBeachFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'spam-past-bayes-svm'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhamCount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspamCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspamWordDict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhamWordDict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'hamCount' is not defined"
     ]
    }
   ],
   "source": [
    "(hamCount, spamCount, labelDict) = preProcessingLabel()\n",
    "NBeachFile('TEST_01327.eml',hamCount,spamCount, vocabulary,spamWordDict,hamWordDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from collections import OrderedDict\n",
    "# preprocessing label\n",
    "def preProcessingLabelSVM(labelFile = \"SPAM.label\"):\n",
    "    labelDict = {}\n",
    "    hamCount = 0\n",
    "    spamCount = 0\n",
    "    with open(\"SPAM.label\") as f:\n",
    "        for line in f:\n",
    "            (val, key) = line.split()\n",
    "            labelDict[key] = int(val)\n",
    "    f.close()\n",
    "    return labelDict\n",
    "### Preprocessing: building vocabulary + frequency:\n",
    "def SVMBuildingMatrix(trainingDir='processed_training'):\n",
    "    # read spam label to identify which doc is spam\n",
    "    labelDict = preProcessingLabelSVM()\n",
    "    # for each file in the training directory\n",
    "    onlyfiles = [f for f in listdir(trainingDir) if isfile(join(trainingDir, f))]\n",
    "    wordList = {}                                       # vocabulary\n",
    "    trainMatrix = {}                                    # trainingMatrix\n",
    "    stop_words = set(stopwords.words('english'))        # english stop words\n",
    "    for i in onlyfiles:\n",
    "        # see some random-file\n",
    "        if i == '.DS_Store': #ignore this file\n",
    "            continue\n",
    "        trainMatrix[i] = {}\n",
    "        inputFile = trainingDir+'/'+i\n",
    "        s = open(inputFile,encoding=\"latin-1\").read()\n",
    "        tokens = s.split()\n",
    "        for word in tokens:\n",
    "            ### Processing word here##################\n",
    "            # ignore stopwords\n",
    "            if word in stop_words:\n",
    "                continue\n",
    "            if word in ':;.,/\\(\\)\\[\\]<>': # some normal character\n",
    "                continue\n",
    "            lmWord = word # was thinking about lemmatize word\n",
    "            ##########################################\n",
    "            ### Update vocabulary#####################\n",
    "            if lmWord in wordList:\n",
    "                # update vector for each document\n",
    "                wordList[lmWord]+=1\n",
    "                if lmWord in trainMatrix:\n",
    "                    trainMatrix[i][lmWord] += 1\n",
    "                else:\n",
    "                    trainMatrix[i][lmWord] = 1\n",
    "            else:\n",
    "                # add new word to word list\n",
    "                wordList[lmWord]=1\n",
    "                if lmWord in trainMatrix:\n",
    "                    trainMatrix[i][lmWord] += 1\n",
    "                else:\n",
    "                    trainMatrix[i][lmWord] = 1\n",
    "                # update vector for each document\n",
    "            ##########################################\n",
    "    return wordList,trainMatrix\n",
    "\n",
    "### get top attribute from wordList, by default using 1000 attribute\n",
    "def getTopAttribute(wordList, n=1000, order=False):\n",
    "    ### get top words \n",
    "    top = sorted(wordList.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    if order:\n",
    "        return OrderedDict(top)\n",
    "    return dict(top)\n",
    "### Output training matrix with number of attribute\n",
    "def getMatrixWithN(Matrix,wordList,n=1000):\n",
    "    # get top attribute\n",
    "    topDict = getTopAttribute(wordList,n,True)\n",
    "    # read spam label to identify which doc is spam\n",
    "    labelDict = preProcessingLabelSVM()\n",
    "    # creating training Matrix\n",
    "    dimension = (len(Matrix),len(topDict))   \n",
    "    processedMatrix=np.zeros(dimension)\n",
    "    labelTrainVector=np.zeros(len(Matrix))\n",
    "    x = 0 # row\n",
    "    y = 0 # column\n",
    "    for doc in Matrix:\n",
    "        for word in topDict:\n",
    "            if word not in Matrix[doc]:\n",
    "                y+=1 # move to next column\n",
    "                continue\n",
    "            else:\n",
    "                processedMatrix[x][y] = Matrix[doc][word]\n",
    "                y+=1 # move to next column\n",
    "        labelTrainVector[x] = labelDict[doc]\n",
    "        x+=1\n",
    "        y=0\n",
    "    return processedMatrix,labelTrainVector\n",
    "### computing stat\n",
    "def SVMComputeStat(prediction, labelTestVector):\n",
    "    if len(prediction) != len(labelTestVector):\n",
    "        print('[-] 2 vectors are not in the same length')\n",
    "        return\n",
    "    tp = 0 # true positive\n",
    "    fp = 0 # false postive\n",
    "    fn = 0 # false negative\n",
    "    tn = 0 # true negative\n",
    "    vectorLength = len(prediction)\n",
    "    for i in range(vectorLength):\n",
    "        if prediction[i] == 1 and prediction[i] == labelTestVector[i]:\n",
    "            tp += 1\n",
    "        elif prediction[i] == 1 and prediction[i] != labelTestVector[i]:\n",
    "            fp += 1\n",
    "        elif prediction[i] == 0 and prediction[i] == labelTestVector[i]:\n",
    "            tn += 1\n",
    "        elif prediction[i] == 0 and prediction[i] != labelTestVector[i]:\n",
    "            fn += 1\n",
    "        else:\n",
    "            print(i,'[-] something is wrong')\n",
    "#     print('tp =',tp,', tn =',tn, ', fp =', fp, ', fn =', fn)\n",
    "    print('False Positive Rate:\\t', fp)\n",
    "    print('False Negative Rate:\\t', fn)\n",
    "    precision = tp/(tp+fp)\n",
    "    recall = tp/(tp+fn)\n",
    "    print('Recall:\\t\\t\\t', recall)    \n",
    "    print('Precision:\\t\\t',precision)\n",
    "    fscore = 2*precision*recall/(precision + recall)\n",
    "    print('F-score beta = 1: \\t', fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[+] SVM on testing data')\n",
    "(wordList,trainMatrix) = SVMBuildingMatrix('processed_training')\n",
    "(testWordList,testMatrix) = SVMBuildingMatrix('processed_testing') #we don't care about test wordlist\n",
    "(processedTrainMatrix, labelTrainVector) = getMatrixWithN(trainMatrix,wordList,300)\n",
    "(processedTestMatrix, labelTestVector) = getMatrixWithN(testMatrix,wordList,300)\n",
    "clf = svm.SVC()\n",
    "clf.fit(processedTrainMatrix, labelTrainVector)  \n",
    "prediction = clf.predict(processedTestMatrix)\n",
    "SVMComputeStat(prediction, labelTestVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[+] 5-fold Cross Validation')\n",
    "### 5-fold validation here:\n",
    "(wordList,trainMatrix) = SVMBuildingMatrix('processed_training')\n",
    "(processedTrainMatrix, labelTrainVector) = getMatrixWithN(trainMatrix,wordList,300)\n",
    "# partion training matrix and label vector\n",
    "partitionMatrices = np.split(processedTrainMatrix, 5)\n",
    "partitionVectors = np.split(labelTrainVector, 5)\n",
    "# First fold:\n",
    "print('[First fold]')\n",
    "# Consider first-fold as test matrix\n",
    "firstFoldMatrix = partitionMatrices[0]\n",
    "firstFoldVector = partitionVectors[0]\n",
    "# Consider last 4-fold as training matrix\n",
    "last4Matricies    = np.concatenate((partitionMatrices[1],partitionMatrices[2],partitionMatrices[3],partitionMatrices[4]), axis=0)\n",
    "last4FoldsVectors = np.concatenate((partitionVectors[1],partitionVectors[2],partitionVectors[3],partitionVectors[4]), axis=0)\n",
    "# test\n",
    "clf = svm.SVC()\n",
    "clf.fit(last4Matricies, last4FoldsVectors)  \n",
    "fivefoldprediction = clf.predict(firstFoldMatrix)\n",
    "SVMComputeStat(fivefoldprediction, firstFoldVector)\n",
    "\n",
    "print('[Second fold]')\n",
    "# Consider first-fold as test matrix\n",
    "firstFoldMatrix = partitionMatrices[1]\n",
    "firstFoldVector = partitionVectors[1]\n",
    "# Consider last 4-fold as training matrix\n",
    "last4Matricies    = np.concatenate((partitionMatrices[0],partitionMatrices[2],partitionMatrices[3],partitionMatrices[4]), axis=0)\n",
    "last4FoldsVectors = np.concatenate((partitionVectors[0],partitionVectors[2],partitionVectors[3],partitionVectors[4]), axis=0)\n",
    "# test\n",
    "clf = svm.SVC()\n",
    "clf.fit(last4Matricies, last4FoldsVectors)  \n",
    "fivefoldprediction = clf.predict(firstFoldMatrix)\n",
    "SVMComputeStat(fivefoldprediction, firstFoldVector)\n",
    "\n",
    "print('[Third fold]')\n",
    "# Consider first-fold as test matrix\n",
    "firstFoldMatrix = partitionMatrices[2]\n",
    "firstFoldVector = partitionVectors[2]\n",
    "# Consider last 4-fold as training matrix\n",
    "last4Matricies    = np.concatenate((partitionMatrices[0],partitionMatrices[1],partitionMatrices[3],partitionMatrices[4]), axis=0)\n",
    "last4FoldsVectors = np.concatenate((partitionVectors[0],partitionVectors[1],partitionVectors[3],partitionVectors[4]), axis=0)\n",
    "# test\n",
    "clf = svm.SVC()\n",
    "clf.fit(last4Matricies, last4FoldsVectors)  \n",
    "fivefoldprediction = clf.predict(firstFoldMatrix)\n",
    "SVMComputeStat(fivefoldprediction, firstFoldVector)\n",
    "\n",
    "print('[Fourth fold]')\n",
    "# Consider first-fold as test matrix\n",
    "firstFoldMatrix = partitionMatrices[3]\n",
    "firstFoldVector = partitionVectors[3]\n",
    "# Consider last 4-fold as training matrix\n",
    "last4Matricies    = np.concatenate((partitionMatrices[0],partitionMatrices[1],partitionMatrices[2],partitionMatrices[4]), axis=0)\n",
    "last4FoldsVectors = np.concatenate((partitionVectors[0],partitionVectors[1],partitionVectors[2],partitionVectors[4]), axis=0)\n",
    "# test\n",
    "clf = svm.SVC()\n",
    "clf.fit(last4Matricies, last4FoldsVectors)  \n",
    "fivefoldprediction = clf.predict(firstFoldMatrix)\n",
    "SVMComputeStat(fivefoldprediction, firstFoldVector)\n",
    "\n",
    "print('[Fifth fold]')\n",
    "# Consider first-fold as test matrix\n",
    "firstFoldMatrix = partitionMatrices[4]\n",
    "firstFoldVector = partitionVectors[4]\n",
    "# Consider last 4-fold as training matrix\n",
    "last4Matricies    = np.concatenate((partitionMatrices[0],partitionMatrices[1],partitionMatrices[2],partitionMatrices[3]), axis=0)\n",
    "last4FoldsVectors = np.concatenate((partitionVectors[0],partitionVectors[1],partitionVectors[2],partitionVectors[3]), axis=0)\n",
    "# test\n",
    "clf = svm.SVC()\n",
    "clf.fit(last4Matricies, last4FoldsVectors)  \n",
    "fivefoldprediction = clf.predict(firstFoldMatrix)\n",
    "SVMComputeStat(fivefoldprediction, firstFoldVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = np.concatenate((test[0], test[1],test[2],test[3],test[4]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processedTrainMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last4FoldsVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
