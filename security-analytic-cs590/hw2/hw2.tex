\documentclass{article}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage[dvipsnames]{xcolor}
\usepackage[colorlinks=true,urlcolor=ForestGreen]{hyperref}
\newcommand{\dspace}{\baselineskip 16pt}
\newcommand{\sspace}{\baselineskip 14pt}
\textheight 9in
\textwidth 6.5in 
\oddsidemargin -0.1in \evensidemargin -0.1in
\topmargin -0.3in
\pagestyle{empty}


\newcounter{problem}[section]
\newenvironment{problem}[1][]{\refstepcounter{problem}\par\medskip
   \noindent \textbf{Problem~\theproblem. #1} \rmfamily}{\medskip}

\newcounter{task}[section]
\newenvironment{task}[1][]{\refstepcounter{task}\par\medskip
   \noindent \textbf{Task~\thetask. #1} \rmfamily}{\medskip}

\newenvironment{proof}{\begin{mdframed}\textbf{Ans:}}{ \end{mdframed}}


\begin{document}
\sspace
\noindent
Purdue University \hfill Duc Viet Le\\
CS 59000SA        \hfill 
% \href{mailto:le52@purdue.edu}{le52@purdue.edu}
\dspace
\begin{center}
{\bf Assignment 2}
\end{center}
\vspace{.2in}

\noindent\textbf{Task 3.1}
\\
\textbf{Note:} in both tasks, I removed last 1327 records in the training folder because it's duplicate with testing folder.
\begin{itemize}
	\item 
	\textbf{Naive Bayes:}
	In naive Bayes, I have implemented the same algorithm covered in the lecture. The only modification from the lectures is that I removed all stopwords (e.g ``the'', ``an'', ``a'', \dots). The following data is my algorithm's performance on test data:
	\begin{verbatim}
		False Positive Rate:  28
		False Negative Rate:  8
		Recall:               0.9911699779249448
		Precision:            0.9697624190064795
		F-score beta = 1:     0.980349344978166
	\end{verbatim}

	\item 
	\textbf{SVM:} In SVM, I used the default SVM modeled provided by the library. I used 300 attributes for training and testing. The following data is the algorithm's performance on testing data:
	\begin{verbatim}
	[+] SVM on testing data
	False Positive Rate:     37
	False Negative Rate:     16
	Recall:                  0.9823399558498896
	Precision:               0.9600862998921251
	F-score beta = 1:        0.9710856519367157
	\end{verbatim}
\end{itemize}

\noindent\textbf{Task 3.2}
\begin{itemize}
	\item Evaluate the results of both algorithm, which one is better from your point of view and why?\\
	\textbf{Ans. }
	I think both algorithms perform well on testing data. From my point of views, I think naive Bayes is simpler, and the performance is somewhat better than SVM. Moreover, naive Bayes's algorithm performs better as the training data increases. 

	However, In the case of spam filter, SVM is also good in the sense that it only needs around 300 attributes to give a good classifiers, so I think SVM is more efficient in term of computation compared to naive Bayes. However, it may not be true for other classifiers.

	\item  Is your model overfiting/underfiting?  How do you know whether your model is overfiting/underfiting or not?
	\\
	\textbf{Ans. }
	For Naive bayes, I think the model is neither overfitting or underfitting because there is no other ``tweaks'' other than removing stopwords, and the performance of the classifier on testing data is good.
	\\
	For SVM, I think my model is overfitting a little bit because I choose the number of attribute that yields a good $F$ score on testing data. 
\end{itemize}
\noindent \textbf{Task 3.3}
Cross Validation: Make a 5-fold cross validation for your SVM. Briefly explain how you prepare the
5-fold dataset and the result\\
\textbf{Ans.} In cross validation, I partition the training data in two 5 subsets:
\begin{verbatim}
	partitionMatrices = np.split(processedTrainMatrix, 5)
\end{verbatim}
Each time I used the other 4 subsets as training data, and evaluate the classifier against the last subset. The following data is the result for each fold:
\begin{verbatim}
    [+] 5-fold Cross Validation
    [First fold]
    False Positive Rate:     35
    False Negative Rate:	 9
    Recall:                  0.9772151898734177
    Precision:               0.9168646080760094
    F-score beta = 1:        0.946078431372549
    [Second fold]
    False Positive Rate:     21
    False Negative Rate:     9
    Recall:                  0.9783132530120482
    Precision:               0.9508196721311475
    F-score beta = 1:        0.9643705463182898
    [Third fold]
    False Positive Rate:     27
    False Negative Rate:     8
    Recall:                  0.9809069212410502
    Precision:               0.9383561643835616
    F-score beta = 1:        0.9591598599766628
    [Fourth fold]
    False Positive Rate:     19
    False Negative Rate:     8
    Recall:                  0.9801980198019802
    Precision:               0.9542168674698795
    F-score beta = 1:        0.967032967032967
    [Fifth fold]
    False Positive Rate:     22
    False Negative Rate:     6
    Recall:                  0.9853658536585366
    Precision:               0.9483568075117371
    F-score beta = 1:        0.9665071770334929
\end{verbatim}
\noindent\textbf{Task 3.4}  For each of the algorithm, try to come up with an spam instance with word ”get”, ”free”, and ”iphone”
which can circumvent the detection and describe how you construct such instance.
\textbf{Ans. } the instance of the spam email is in \texttt{duc-spam} folder. For naive Bayes, if we keep the body short with limited instance of spam word such as \texttt{free, iphone} and try not to use characteristics of spam email such as all upper case character, mispell, etc. 

For SVM i think, it should be easier because we only need to create an instance that either close to a ham email or contains words that are not in attributes used for computation.
\end{document}